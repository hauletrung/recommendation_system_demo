{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "headers = ['user_id', 'game', 'behavior', 'hours_played', 'other_columns']\n",
    "# Load the dataset\n",
    "data = pd.read_csv('steam-200k.csv', header=None, names=headers)\n",
    "\n",
    "# Data cleaning\n",
    "data.dropna(inplace=True)\n",
    "data.drop_duplicates(inplace=True)\n",
    "\n",
    "# Filter games and users with sufficient interactions\n",
    "data = data[data['hours_played'] > 0]\n",
    "\n",
    "# Split into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "# Create a bipartite graph\n",
    "G = nx.Graph()\n",
    "\n",
    "# Add nodes and edges\n",
    "for row in train_data.itertuples():\n",
    "    G.add_node(row.user_id, bipartite=0)\n",
    "    G.add_node(row.game, bipartite=1)\n",
    "    G.add_edge(row.user_id, row.game, weight=row.hours_played)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/haule/Documents/Studying/UIT/Mang Xa Hoi/GameRecommendationSystem/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Computing transition probabilities: 100%|██████████| 17041/17041 [01:31<00:00, 185.88it/s]\n",
      "Generating walks (CPU: 2): 100%|██████████| 250/250 [3:03:55<00:00, 44.14s/it]  \n",
      "Generating walks (CPU: 4): 100%|██████████| 250/250 [3:04:15<00:00, 44.22s/it]\n",
      "Generating walks (CPU: 1): 100%|██████████| 250/250 [3:04:53<00:00, 44.37s/it]\n",
      "Generating walks (CPU: 3): 100%|██████████| 250/250 [3:07:30<00:00, 45.00s/it]\n"
     ]
    }
   ],
   "source": [
    "from node2vec import Node2Vec\n",
    "\n",
    "# Generate random walks and learn embeddings\n",
    "node2vec = Node2Vec(G, dimensions=64, walk_length=60, num_walks=1000, workers=4)\n",
    "model = node2vec.fit(window=10, min_count=1, batch_words=4)\n",
    "\n",
    "# Get embeddings for all nodes\n",
    "embeddings = model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "model.save('node2vec_model_walk_1000_length_60')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Nancy Drew Ghost Dogs of Moon Lake ', 'Amnesia The Dark Descent', 'All Zombies Must Die!', 'Battlepaths', 'Jurassic Park The Game']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def recommend(user_id, num_recommendations=5):\n",
    "    user_id_str = str(user_id)\n",
    "    if user_id_str not in embeddings:\n",
    "        print(f\"User ID {user_id} was not found in embeddings\")\n",
    "        return []\n",
    "    user_embedding = embeddings[str(user_id)]\n",
    "\n",
    "    # Filter item nodes (those with bipartite=1)\n",
    "    item_nodes = [node for node, data in G.nodes(data=True) if data.get('bipartite') == 1]\n",
    "    \n",
    "    # Compute similarity between user embedding and all item embeddings\n",
    "    item_embeddings = [embeddings[str(item)] for item in item_nodes if str(item) in embeddings]\n",
    "    similarities = cosine_similarity([user_embedding], item_embeddings).flatten()\n",
    "    \n",
    "    # Get top N most similar items\n",
    "    top_indices = similarities.argsort()[-num_recommendations:][::-1]\n",
    "    recommended_items = [item_nodes[i] for i in top_indices]\n",
    "    \n",
    "    return recommended_items\n",
    "\n",
    "sample_user_id = 23717586\n",
    "# Example recommendation\n",
    "recommendations = recommend(user_id=sample_user_id, num_recommendations=5)\n",
    "print(recommendations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Mean Squared Error: 18665.57929391597\n",
      "Random Mean Squared Error: 30771769.723452054\n",
      "embeddings KeyedVectors<vector_size=64, 17041 keys>\n",
      "Mean Squared Error of Node2Vec Model: 18819.268193589043\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def evaluate(test_data):\n",
    "    predictions, actuals = [], []\n",
    "    for row in test_data.itertuples():\n",
    "        user_id, game_id = str(row.user_id), str(row.game)\n",
    "        if user_id in embeddings and game_id in embeddings:\n",
    "            user_embedding = embeddings[user_id]\n",
    "            game_embedding = embeddings[game_id]\n",
    "            prediction = cosine_similarity([user_embedding], [game_embedding]).flatten()[0]\n",
    "            predictions.append(prediction)\n",
    "            actuals.append(row.hours_played)\n",
    "        # else:\n",
    "            # print(f\"Skipping missing ID: User {user_id}, Game {game_id}\")\n",
    "    \n",
    "    if not predictions:  # Handle case where no valid predictions are made\n",
    "        return float('inf')\n",
    "    \n",
    "    mse = mean_squared_error(actuals, predictions)\n",
    "    return mse\n",
    "\n",
    "# Calculate the mean rating from the test set\n",
    "mean_rating = test_data['hours_played'].mean()\n",
    "\n",
    "# Predict all ratings as the mean rating\n",
    "baseline_predictions = [mean_rating] * len(test_data)\n",
    "\n",
    "# Calculate MSE for the baseline model\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "baseline_mse = mean_squared_error(test_data['hours_played'], baseline_predictions)\n",
    "print(f'Baseline Mean Squared Error: {baseline_mse}')\n",
    "\n",
    "# Generate random predictions within the range of actual ratings\n",
    "min_rating = test_data['hours_played'].min()\n",
    "max_rating = test_data['hours_played'].max()\n",
    "random_predictions = np.random.uniform(min_rating, max_rating, size=len(test_data))\n",
    "\n",
    "# Calculate MSE for random predictions\n",
    "random_mse = mean_squared_error(test_data['hours_played'], random_predictions)\n",
    "print(f'Random Mean Squared Error: {random_mse}')\n",
    "\n",
    "mse = evaluate(test_data)\n",
    "print(f'Mean Squared Error of Node2Vec Model: {mse}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (.venv)",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
